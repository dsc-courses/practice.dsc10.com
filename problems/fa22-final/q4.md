# BEGIN PROB

In `apps`, our sample of 1,000 credit card applications, applicants who were approved for the credit card have fewer dependents, on average, than applicants who were denied. The mean number of dependents for approved applicants is 0.98, versus 1.07 for denied applicants.

To test whether this difference is purely due to random chance, or whether the distributions of the number of dependents for approved and denied applicants are truly different in the population of all credit card applications, we decide to perform a permutation test.

Consider the incomplete code block below.

```py
def shuffle_status(df):
    shuffled_status = np.random.permutation(df.get("status"))
    return df.assign(status=shuffled_status).get(["status", "dependents"])

def test_stat(df):
    grouped = df.groupby("status").mean().get("dependents")
    approved = grouped.loc["approved"]
    denied = grouped.loc["denied"]
    return __(a)__

stats = np.array([])
for i in np.arange(10000):
    shuffled_apps = shuffle_status(apps)
    stat = test_stat(shuffled_apps)
    stats = np.append(stats, stat)

p_value = np.count_nonzero(__(b)__) / 10000
```

Below are six options for filling in blanks (a) and (b) in the code above.

<center>
<table class="table" style="width:50%">
  <thead>
    <tr>
      <th scope="col"></th>
      <th scope="col">Blank (a)</th>
      <th scope="col">Blank (b)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th scope="row">Option 1</th>
      <td><code>denied - approved</code></td>
      <td><code>stats >= test_stat(apps)</code></td>
    </tr>
    <tr>
      <th scope="row">Option 2</th>
      <td><code>denied - approved</code></td>
      <td><code>stats <= test_stat(apps)</code></td>
    </tr>
    <tr>
      <th scope="row">Option 3</th>
      <td><code>approved - denied</code></td>
      <td><code>stats >= test_stat(apps)</code></td>
    </tr>
    <tr>
      <th scope="row">Option 4</th>
      <td><code>np.abs(denied - approved)</code></td>
      <td><code>stats >= test_stat(apps)</code></td>
    </tr>
    <tr>
      <th scope="row">Option 5</th>
      <td><code>np.abs(denied - approved)</code></td>
      <td><code>stats <= test_stat(apps)</code></td>
    </tr>
    <tr>
      <th scope="row">Option 6</th>
      <td><code>np.abs(approved - denied)</code></td>
      <td><code>stats >= test_stat(apps)</code></td>
    </tr>
  </tbody>
</table>
</center>

The correct way to fill in the blanks depends on how we choose our null and alternative hypotheses. 


# BEGIN SUBPROB

Suppose we choose the following pair of hypotheses.

- **Null Hypothesis**: In the population, the number of dependents of approved and denied applicants come from the same distribution.

- **Alternative Hypothesis**: In the population, the number of dependents of approved applicants and denied applicants do not come from the same distribution.

Which of the six presented options could correctly fill in blanks (a) and (b) for this pair of hypotheses? Select all that apply.

[ ] Option 1
[ ] Option 2
[ ] Option 3
[ ] Option 4
[ ] Option 5
[ ] Option 6
[ ] None of the above.

# BEGIN SOLUTION

**Answer: ** Option 4, Option 6

For blank (a), we want to choose a test statistic that helps us distinguish between the null and alternative hypotheses. The alternative hypothesis says that `denied` and `approved` should be different, but it doesn't say which should be larger. Options 1 through 3 therefore won't work, because high values and low values of these statistics both point to the alternative hypothesis, and moderate values point to the null hypothesis. Options 4 through 6 all work because large values point to the alternative hypothesis, and small values close to 0 suggest that the null hypothesis should be true.

For blank (b), we want to calculate the p-value in such a way that it represents the proportion of trials for which the simulated test statistic was equal to the observed statistic or further in the direction of the alternative. For all of Options 4 through 6, large values of the test statistic indicate the alternative, so we need to calculate the p-value with a `>=` sign, as in Options 4 and 6. 

While Option 3 filled in blank (a) correctly, it did not fill in blank (b) correctly. Options 4 and 6 fill in both blanks correctly.

<average>78</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Now, suppose we choose the following pair of hypotheses.

- **Null Hypothesis**: In the population, the number of dependents of approved and denied applicants come from the same distribution.

- **Alternative Hypothesis**: In the population, the number of dependents of approved applicants is smaller on average than the number of dependents of denied applicants.

Which of the six presented options could correctly fill in blanks (a)
and (b) for this pair of hypotheses? Select all that apply.

# BEGIN SOLUTION

**Answer: ** Option 1

As in the previous part, we need to fill blank (a) with a test statistic such that large values point towards one of the hypotheses and small values point towards the other. Here, the alterntive hypothesis suggests that `approved` should be less than `denied`, so we can't use Options 4 through 6 because these can only detect whether `approved` and `denied` are not different, not which is larger. Any of Options 1 through 3 should work, however. For Options 1 and 2, large values point towards the alternative, and for Option 3, small values point towards the alternative. This means we need to calculate the p-value in blank (b) with a `>=` symbol for the test statistic from Options 1 and 2, and a `<=` symbol for the test statistic from Option 3. Only Options 1 fills in blank (b) correctly based on the test statistic used in blank (a).

<average>83</average>

# END SOLUTION

# END SUBPROB

# BEGIN SUBPROB

Option 6 from the start of this question is repeated below.

<center>
<table class="table" style="width:50%">
  <thead>
    <tr>
      <th scope="col"></th>
      <th scope="col">Blank (a)</th>
      <th scope="col">Blank (b)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th scope="row">Option 6</th>
      <td><code>np.abs(approved - denied)</code></td>
      <td><code>stats >= test_stat(apps)</code></td>
    </tr>
  </tbody>
</table>
</center>

We want to create a new option, Option 7, that replicates the behavior
of Option 6, but with blank (a) filled in as shown:

<center>
<table class="table" style="width:50%">
  <thead>
    <tr>
      <th scope="col"></th>
      <th scope="col">Blank (a)</th>
      <th scope="col">Blank (b)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th scope="row">Option 7</th>
      <td><code>approved - denied</code></td>
      <td></td>
    </tr>
  </tbody>
</table>
</center>

Which expression below could go in blank (b) so that Option 7 is
equivalent to Option 6?

( ) `np.abs(stats) >= test_stat(apps)`
( ) `stats >= np.abs(test_stat(apps))`
( ) `np.abs(stats) >= np.abs(test_stat(apps))`
( ) `np.abs(stats >= test_stat(apps))`

# BEGIN SOLUTION

**Answer: ** `np.abs(stats) >= np.abs(test_stat(apps))`

First, we need to understand how Option 6 works. Option 6 produces large values of the test statistic when `approved` is very different from `denied`, then calculates the p-value as the proportion of trials for which the simulated test statistic was larger than the observed statistic. In other words, Option 6 calculates the proportion of trials in which `approved` and  `denied` are more different in a pair of random samples than they are in the original samples.

For Option 7, the test statistic for a pair of random samples may come out very large or very small when `approved` is very different from `denied`. Similarly, the observed statistic may come out very large or very small when `approved` and `denied` are very different in the original samples. We want to find the  proportion of trials in which `approved` and  `denied` are more different in a pair of random samples than they are in the original samples, which means we want the proportion of trials in which the absolute value of `approved - denied` in a pair of random samples is larger than the absolute value of `approved - denied` in the original samples.

<average>56</average>

# END SOLUTION

# END SUBPROB



# BEGIN SUBPROB

In our implementation of this permutation test, we followed the
procedure outlined in lecture to draw new pairs of samples under the
null hypothesis and compute test statistics --- that is, we randomly
assigned each row to a group (approved or denied) by shuffling one of
the columns in `apps`, then computed the test statistic on this random
pair of samples.

Let's now explore an alternative solution to drawing pairs of samples
under the null hypothesis and computing test statistics. Here's the
approach:

1. Shuffle, i.e. re-order, the rows of the DataFrame.
2. Use the values at the top of the resulting `"dependents"` column as the new "denied" sample, and the values at the at the bottom of the resulting `"dependents"` column as the new "approved" sample. Note that we donâ€™t necessarily split the DataFrame exactly in half --- the sizes of these new samples depend on the number of "denied" and "approved" values in the original DataFrame!

Once we generate our pair of random samples in this way, we'll compute the test statistic on the random pair, as usual. Here, we'll use as our test statistic the difference between the mean number of dependents for denied and approved applicants, in the order **denied minus approved**.

**Fill in the blanks to complete the simulation below.**

*Hint:* `np.random.permutation` shouldn't appear anywhere in your code.

```py
    def shuffle_all(df):
        '''Returns a DataFrame with the same rows as df, but reordered.'''
        return __(a)__

    def fast_stat(df):
        # This function does not and should not contain any randomness.
        denied = np.count_nonzero(df.get("status") == "denied")
        mean_denied = __(b)__.get("dependents").mean()
        mean_approved = __(c)__.get("dependents").mean()
        return mean_denied - mean_approved

    stats = np.array([])
    for i in np.arange(10000):
        stat = fast_stat(shuffle_all(apps))
        stats = np.append(stats, stat)
```

# BEGIN SOLUTION
**Answer: ** The blanks should be filled in as follows:

- (a) `df.sample(df.shape[0])`
- (b) `df.take(np.arange(denied))`
- (c) `df.take(np.arange(denied, df.shape[0]))`

For blank (a), we are told to return a DataFrame with the same rows but in a different order. We can use the `.sample` method for this question. We want each row of the input DataFrame `df` to appear once, so we should sample without replacement, and we should have has many rows in the output as in `df`, so our sample should be of size `df.shape[0]`. Since sampling without replacement is the default behavior of `.sample`, it is optional to specify `replace=False`.

<average>59</average>
<br><br>

For blank (b), we need to implement the strategy outlined, where after we shuffle the DataFrame, we use the values at the top of the DataFrame as our new "denied sample. In a permutation test, the two random groups we create should have the same sizes as the two original groups we are given. In this case, the size of the "denied" group in our original data is stored in the variable `denied`. So we need the rows in positions 0, 1, 2, ..., `denied - 1`, which we can get using `df.take(np.arange(denied))`. 

<average>39</average>
<br><br>

For blank (c), we need to get all remaining applicants, who form the new "approved" sample. We can `.take` the rows corresponding to the ones we didn't put into the "denied" group. That is, the first applicant who will be put into this group is at position `denied`, and we'll take all applicants from there onwards. We should therefore fill in blank (c) with `df.take(np.arange(denied, df.shape[0]))`.

For example, if `apps` had only 10 rows, 7 of them corresponding to denied applications, we would shuffle the rows of `apps`, then take rows 0, 1, 2, 3, 4, 5, 6 as our new "denied" sample and rows 7, 8, 9 as our new "approved" sample.

<average>38</average>

# END SOLUTION

# END SUBPROB

# END PROB
